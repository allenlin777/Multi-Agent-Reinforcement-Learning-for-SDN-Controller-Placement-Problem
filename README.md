# Multi-Agent Reinforcement Learning for SDN Controller Placement Problem
NYCU Computer Science and Engineering Projects

一、專題簡介與相關研究

軟體定義網路有三層架構，分別是應用層、控制層、資料層。控制層的控制器就像是整體網路的大腦，管理著資料層的交換機。而整體網路的效能受到不同控制器跟交換機的延遲時間、控制器的最大負載容量等等因素所影響。而我們專題探討在滿足所有限制下，為了節省成本，如何最小化控制器的佈署數量並得到其佈署位置。先前已有論文提出賽局理論解法。賽局解必能找到可行解法，然而不一定是最佳解；且論文中的效能最好的方法收斂時間較長，只能在小規模拓撲下運行。因此我們提出多智能體強化學習方法(Multi-AgentReinforcement Learning, MARL)，此方法能讓每個智能體從環境的回饋中學習，經由訓練後找到最佳的佈署組合解，並運用在大規模拓撲中。

二、研究方法

我們使用美國城市模擬交換機的節點，並以地理距離模擬延遲，設定每個交換器需要 T個控制器共同管理，我們的目標是佈署最少的控制器來涵蓋所有節點上的 N 台交換機。

<img width="1232" height="708" alt="image" src="https://github.com/user-attachments/assets/fcf69717-895e-44a7-8b1c-457de9f26a1c" />

我們使用 Q-learning 來訓練所有控制器，每個控制器皆為一個智能體，且同時只有一個智能體行動。智能體不需要擁有動態環境或對手的模型，並在探索環境中維護 Q-table，一維為狀態(state)，另一維為動作(Action)。每個狀態對應2 × n的陣列，第一列列出此控制器對每個交換器是否連線，第二列為每個交換器目前是否需要改變以達成共同管理需求。而每個動作則為一個數字 i，i=1~N 時需要改變控制器對 i 交換器的連接，i=0 時此控制器不須對任何交換器改變連接。Q-value 則是從環境回饋的獎賞函數(Reward)中更新，獎賞函數參考了論文中的效用函數(Utility)，效用函數根據是否超過共同管理數量、最大負載容量、延遲時間來獎勵或懲罰智能體，而我們也加上了總佈署數量，獎勵較低的佈署數量，以訓練出滿足所有限制的最小佈署組合。

三、實驗結果

為了公平比較兩者差異，我們設 T=1 並對比進行 100 次賽局與經過 3000 次訓練模型的100 次測試，以平均控制器數量為衡量指標。賽局解中效能最好的 PCPP 為 4.09，次好的Quick-PCPP 為 5.38，我們的 MARL 為 4.91。雖然離最好的 PCPP 有一段差距，不過 MARL 可以在大規模拓撲下運行，贏過了也能在此拓撲下運行的 Quick-PCPP，改善控制器佈署在大規模拓撲中的效能。且我們的 MARL 有收斂性質，我想再調整參數、state 數量或是獎勵函數可以進一步減少數量，來更逼近或超越 PCPP。四、未來研究方向由於存每個智能體的 Q-table 需要大量記憶體，因此我們這學期也在繼續進行使用深度強化學習方法 Deep Q-Network(DQN)改善記憶體問題。

<img width="1199" height="552" alt="image" src="https://github.com/user-attachments/assets/ba05e952-b1ce-454f-ba23-62e2771e2ec4" />
